{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f33959",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-20T17:40:13.164678Z",
     "iopub.status.busy": "2026-02-20T17:40:13.164374Z",
     "iopub.status.idle": "2026-02-20T17:40:17.057064Z",
     "shell.execute_reply": "2026-02-20T17:40:17.056245Z"
    },
    "papermill": {
     "duration": 3.898882,
     "end_time": "2026-02-20T17:40:17.058897",
     "exception": false,
     "start_time": "2026-02-20T17:40:13.160015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COLETA DE DADOS - VERSÃƒO SIMPLIFICADA\n",
      "================================================================================\n",
      "\n",
      "ConfiguraÃ§Ã£o:\n",
      "   Ticker: SPY\n",
      "   Data inicial: 2010-01-01\n",
      "   Intervalo: DiÃ¡rio (1d)\n",
      "\n",
      "================================================================================\n",
      "COLETANDO DADOS: SPY\n",
      "================================================================================\n",
      "ğŸ“Š Intervalo: DIÃRIO (1d)\n",
      "ğŸ“… PerÃ­odo: 2010-01-01 atÃ© hoje\n",
      "\n",
      "âœ… Coleta bem-sucedida!\n",
      "   ğŸ“Š Total de dias: 4057\n",
      "   ğŸ“… PerÃ­odo: 2010-01-04 atÃ© 2026-02-19\n",
      "   ğŸ“ˆ Colunas: [('Close', 'SPY'), ('High', 'SPY'), ('Low', 'SPY'), ('Open', 'SPY'), ('Volume', 'SPY')]\n",
      "\n",
      "================================================================================\n",
      "INFORMAÃ‡Ã•ES DO DATASET: SPY\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š DimensÃµes: 4057 linhas Ã— 5 colunas\n",
      "ğŸ“… PerÃ­odo: 2010-01-04 atÃ© 2026-02-19\n",
      "â° DuraÃ§Ã£o: 5890 dias\n",
      "\n",
      "ğŸ“ˆ Primeiras 3 linhas:\n",
      "Price           Close       High        Low       Open     Volume\n",
      "Ticker            SPY        SPY        SPY        SPY        SPY\n",
      "Date                                                             \n",
      "2010-01-04  85.027969  85.072984  83.662480  84.307712  118944600\n",
      "2010-01-05  85.253036  85.290552  84.667828  84.975441  111579900\n",
      "2010-01-06  85.313080  85.523154  85.103005  85.170527  116074400\n",
      "\n",
      "ğŸ“ˆ Ãšltimas 3 linhas:\n",
      "Price            Close        High         Low        Open    Volume\n",
      "Ticker             SPY         SPY         SPY         SPY       SPY\n",
      "Date                                                                \n",
      "2026-02-17  682.849976  684.940002  675.780029  680.140015  81354700\n",
      "2026-02-18  686.289978  689.150024  682.830017  684.020020  73570300\n",
      "2026-02-19  684.479980  686.179993  681.549988  683.840027  58530500\n",
      "\n",
      "ğŸ“Š EstatÃ­sticas:\n",
      "Price         Close         High          Low         Open        Volume\n",
      "Ticker          SPY          SPY          SPY          SPY           SPY\n",
      "count   4057.000000  4057.000000  4057.000000  4057.000000  4.057000e+03\n",
      "mean     272.221906   273.630206   270.583485   272.175706  1.091458e+08\n",
      "std      159.909774   160.693962   158.988378   159.896257  6.775048e+07\n",
      "min       77.359543    78.283014    76.549612    78.048363  2.027000e+07\n",
      "25%      148.293732   148.912738   147.699208   148.440333  6.523740e+07\n",
      "50%      231.488541   233.613564   228.968462   231.601093  8.929440e+07\n",
      "75%      390.189453   392.360914   387.634237   389.902290  1.324718e+08\n",
      "max      695.489990   697.840027   693.940002   697.049988  7.178287e+08\n",
      "\n",
      "âš ï¸ Valores ausentes:\n",
      "   âœ… Nenhum valor ausente\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ğŸ’¾ Dados salvos: /kaggle/working/spy_raw.csv\n",
      "\n",
      "================================================================================\n",
      "âœ… COLETA CONCLUÃDA COM SUCESSO!\n",
      "================================================================================\n",
      "ğŸ“ Arquivo salvo: /kaggle/working/spy_raw.csv\n",
      "ğŸ“Š Total de linhas: 4057\n",
      "\n",
      "â¡ï¸ PrÃ³ximo passo: Execute o script de prÃ©-processamento\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "========================================\n",
    "SCRIPT 1: COLETA DE DADOS\n",
    "VersÃ£o Simplificada - APENAS dados DIÃRIOS\n",
    "========================================\n",
    "\"\"\"\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class StockDataCollector:\n",
    "    \"\"\"Coletor simplificado - apenas dados diÃ¡rios\"\"\"\n",
    "    \n",
    "    def __init__(self, symbol='SPY'):\n",
    "        self.symbol = symbol\n",
    "        self.df = None\n",
    "        \n",
    "    def collect_data(self, start_date='2010-01-01'):\n",
    "        \"\"\"\n",
    "        Coleta dados diÃ¡rios desde start_date atÃ© hoje\n",
    "        \n",
    "        ParÃ¢metros:\n",
    "        - start_date: Data inicial (padrÃ£o: 2010-01-01)\n",
    "                     Use '2000-01-01' para mais dados\n",
    "                     Use '2015-01-01' para menos dados\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"COLETANDO DADOS: {self.symbol}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ“Š Intervalo: DIÃRIO (1d)\")\n",
    "        print(f\"ğŸ“… PerÃ­odo: {start_date} atÃ© hoje\")\n",
    "        \n",
    "        # Coletar dados\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        self.df = yf.download(\n",
    "            self.symbol,\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "            interval='1d',\n",
    "            progress=False,\n",
    "            auto_adjust=True  # Ajusta automaticamente por splits/dividendos\n",
    "        )\n",
    "        \n",
    "        # Verificar se conseguiu dados\n",
    "        if len(self.df) == 0:\n",
    "            print(f\"\\nâŒ ERRO: Nenhum dado coletado para {self.symbol}\")\n",
    "            print(f\"   Verifique se o ticker estÃ¡ correto\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nâœ… Coleta bem-sucedida!\")\n",
    "        print(f\"   ğŸ“Š Total de dias: {len(self.df)}\")\n",
    "        print(f\"   ğŸ“… PerÃ­odo: {self.df.index[0].date()} atÃ© {self.df.index[-1].date()}\")\n",
    "        print(f\"   ğŸ“ˆ Colunas: {list(self.df.columns)}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def save_data(self, filename=None):\n",
    "        \"\"\"Salva dados coletados\"\"\"\n",
    "        if self.df is None:\n",
    "            print(\"âš ï¸ Nenhum dado para salvar. Execute collect_data() primeiro.\")\n",
    "            return None\n",
    "        \n",
    "        if filename is None:\n",
    "            filename = f'{self.symbol.lower()}_raw.csv'\n",
    "        \n",
    "        filepath = os.path.join('/kaggle/working/', filename)\n",
    "        self.df.to_csv(filepath)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Dados salvos: {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Mostra informaÃ§Ãµes detalhadas dos dados\"\"\"\n",
    "        if self.df is None:\n",
    "            print(\"âš ï¸ Nenhum dado disponÃ­vel\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"INFORMAÃ‡Ã•ES DO DATASET: {self.symbol}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nğŸ“Š DimensÃµes: {self.df.shape[0]} linhas Ã— {self.df.shape[1]} colunas\")\n",
    "        print(f\"ğŸ“… PerÃ­odo: {self.df.index[0].date()} atÃ© {self.df.index[-1].date()}\")\n",
    "        print(f\"â° DuraÃ§Ã£o: {(self.df.index[-1] - self.df.index[0]).days} dias\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Primeiras 3 linhas:\")\n",
    "        print(self.df.head(3))\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Ãšltimas 3 linhas:\")\n",
    "        print(self.df.tail(3))\n",
    "        \n",
    "        print(f\"\\nğŸ“Š EstatÃ­sticas:\")\n",
    "        print(self.df.describe())\n",
    "        \n",
    "        print(f\"\\nâš ï¸ Valores ausentes:\")\n",
    "        missing = self.df.isnull().sum()\n",
    "        if missing.sum() > 0:\n",
    "            print(missing[missing > 0])\n",
    "        else:\n",
    "            print(\"   âœ… Nenhum valor ausente\")\n",
    "        \n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"FunÃ§Ã£o principal - executa coleta completa\"\"\"\n",
    "    \n",
    "    # ConfiguraÃ§Ã£o\n",
    "    SYMBOL = 'SPY'  # â† MUDE AQUI: 'SPY', 'GOLD', 'MSFT', 'AAPL', etc.\n",
    "    START_DATE = '2010-01-01'  # â† MUDE AQUI para mais/menos dados\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COLETA DE DADOS - VERSÃƒO SIMPLIFICADA\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nConfiguraÃ§Ã£o:\")\n",
    "    print(f\"   Ticker: {SYMBOL}\")\n",
    "    print(f\"   Data inicial: {START_DATE}\")\n",
    "    print(f\"   Intervalo: DiÃ¡rio (1d)\")\n",
    "    \n",
    "    # Criar coletor\n",
    "    collector = StockDataCollector(symbol=SYMBOL)\n",
    "    \n",
    "    # Coletar dados\n",
    "    df = collector.collect_data(start_date=START_DATE)\n",
    "    \n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Mostrar informaÃ§Ãµes\n",
    "    collector.get_info()\n",
    "    \n",
    "    # Salvar\n",
    "    filepath = collector.save_data()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… COLETA CONCLUÃDA COM SUCESSO!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ğŸ“ Arquivo salvo: {filepath}\")\n",
    "    print(f\"ğŸ“Š Total de linhas: {len(df)}\")\n",
    "    print(f\"\\nâ¡ï¸ PrÃ³ximo passo: Execute o script de prÃ©-processamento\")\n",
    "    \n",
    "    return collector\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collector = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac283348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T17:40:17.065530Z",
     "iopub.status.busy": "2026-02-20T17:40:17.064916Z",
     "iopub.status.idle": "2026-02-20T17:40:41.491869Z",
     "shell.execute_reply": "2026-02-20T17:40:41.490873Z"
    },
    "papermill": {
     "duration": 24.431782,
     "end_time": "2026-02-20T17:40:41.493474",
     "exception": false,
     "start_time": "2026-02-20T17:40:17.061692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 17:40:20.388048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771609220.781823      24 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771609220.897060      24 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771609221.872794      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771609221.872844      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771609221.872846      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771609221.872849      24 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "GPU Devices:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "âœ… GPU detectada! TensorFlow usarÃ¡ GPU automaticamente.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Verificar GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Se quiser forÃ§ar uso de GPU\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"âœ… GPU detectada! TensorFlow usarÃ¡ GPU automaticamente.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Nenhuma GPU detectada. Usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0048a993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T17:40:41.500328Z",
     "iopub.status.busy": "2026-02-20T17:40:41.499789Z",
     "iopub.status.idle": "2026-02-20T17:40:41.727562Z",
     "shell.execute_reply": "2026-02-20T17:40:41.726703Z"
    },
    "papermill": {
     "duration": 0.233529,
     "end_time": "2026-02-20T17:40:41.729563",
     "exception": false,
     "start_time": "2026-02-20T17:40:41.496034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PRÃ‰-PROCESSAMENTO - 100% LIVRE DE DATA LEAKAGE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‚ Carregando: /kaggle/working/spy_raw.csv\n",
      "   âœ“ Carregados 4059 registros\n",
      "\n",
      "================================================================================\n",
      "EXECUTANDO PIPELINE\n",
      "================================================================================\n",
      "\n",
      "ğŸ”„ Convertendo colunas para numÃ©rico...\n",
      "   âœ“ Tipos apÃ³s conversÃ£o:\n",
      "      Open: float64\n",
      "      High: float64\n",
      "      Low: float64\n",
      "      Close: float64\n",
      "      Volume: float64\n",
      "\n",
      "   âš ï¸ NaNs apÃ³s conversÃ£o (valores nÃ£o-numÃ©ricos):\n",
      "Open      2\n",
      "High      2\n",
      "Low       2\n",
      "Close     2\n",
      "Volume    2\n",
      "dtype: int64\n",
      "\n",
      "ğŸ”§ Tratando valores ausentes...\n",
      "Valores ausentes ANTES:\n",
      "Close     2\n",
      "High      2\n",
      "Low       2\n",
      "Open      2\n",
      "Volume    2\n",
      "dtype: int64\n",
      "   âœ“ Removidas 2 linhas iniciais com NaN\n",
      "\n",
      "Valores ausentes APÃ“S:\n",
      "   âœ… Nenhum\n",
      "\n",
      "ğŸ“ˆ Criando indicadores tÃ©cnicos...\n",
      "   âœ“ Indicadores criados: 21 colunas\n",
      "   âœ“ Removidas 49 linhas (warm-up)\n",
      "   âœ“ Registros finais: 4008\n",
      "\n",
      "ğŸ¯ Features selecionadas: 21\n",
      "    1. Open\n",
      "    2. High\n",
      "    3. Low\n",
      "    4. Close\n",
      "    5. Volume\n",
      "    6. MA_7\n",
      "    7. MA_21\n",
      "    8. MA_50\n",
      "    9. EMA_12\n",
      "   10. EMA_26\n",
      "   11. MACD\n",
      "   12. MACD_Signal\n",
      "   13. RSI\n",
      "   14. BB_Upper\n",
      "   15. BB_Middle\n",
      "   16. BB_Lower\n",
      "   17. Volume_MA_7\n",
      "   18. Price_Change\n",
      "   19. High_Low_Pct\n",
      "   20. Volatility\n",
      "   21. ATR\n",
      "\n",
      "ğŸ”¢ Criando sequÃªncias...\n",
      "   Lookback: 60 timesteps\n",
      "   âœ“ X shape: (3948, 60, 21)\n",
      "   âœ“ y shape: (3948,)\n",
      "   âœ“ SequÃªncias: 3948\n",
      "\n",
      "âœ‚ï¸ Dividindo dados...\n",
      "   âœ“ Treino: 2566 (65%)\n",
      "   âœ“ Val: 789 (20%)\n",
      "   âœ“ Teste: 593 (15%)\n",
      "\n",
      "ğŸ”„ Normalizando...\n",
      "   âœ… Fit APENAS no treino\n",
      "   âœ… NormalizaÃ§Ã£o completa\n",
      "\n",
      "ğŸ’¾ Salvando dados...\n",
      "   âœ“ 6 arquivos .npy salvos\n",
      "\n",
      "ğŸ’¾ Preprocessor salvo: /kaggle/working/preprocessor.pkl\n",
      "\n",
      "================================================================================\n",
      "âœ… PRÃ‰-PROCESSAMENTO CONCLUÃDO\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Checklist:\n",
      "   âœ… ConversÃ£o para numÃ©rico\n",
      "   âœ… Missing values (forward fill)\n",
      "   âœ… Indicadores (janela passada)\n",
      "   âœ… SequÃªncias (sem leakage)\n",
      "   âœ… Split (temporal)\n",
      "   âœ… NormalizaÃ§Ã£o (fit no treino)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/122579099.py:266: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(data_path, index_col=0, parse_dates=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "========================================\n",
    "SCRIPT 2: PRÃ‰-PROCESSAMENTO (CORRIGIDO)\n",
    "VersÃ£o Limpa - SEM data leakage\n",
    "+ ConversÃ£o de tipos\n",
    "========================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "class StockDataPreprocessor:\n",
    "    \"\"\"PrÃ©-processador limpo e auditado\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.feature_columns = []\n",
    "    \n",
    "    def convert_to_numeric(self):\n",
    "        \"\"\"\n",
    "        âœ… Converte todas as colunas para numÃ©rico\n",
    "        Resolve problema de dados lidos como string\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ”„ Convertendo colunas para numÃ©rico...\")\n",
    "        \n",
    "        numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in self.df.columns:\n",
    "                # Converter para numÃ©rico, erros viram NaN\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "        \n",
    "        # Verificar tipos\n",
    "        print(f\"   âœ“ Tipos apÃ³s conversÃ£o:\")\n",
    "        for col in numeric_columns:\n",
    "            if col in self.df.columns:\n",
    "                print(f\"      {col}: {self.df[col].dtype}\")\n",
    "        \n",
    "        # Verificar NaN apÃ³s conversÃ£o\n",
    "        nan_count = self.df[numeric_columns].isnull().sum()\n",
    "        if nan_count.sum() > 0:\n",
    "            print(f\"\\n   âš ï¸ NaNs apÃ³s conversÃ£o (valores nÃ£o-numÃ©ricos):\")\n",
    "            print(nan_count[nan_count > 0])\n",
    "        else:\n",
    "            print(f\"   âœ“ Sem NaNs apÃ³s conversÃ£o\")\n",
    "        \n",
    "        return self.df\n",
    "        \n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"\n",
    "        âœ… Trata valores ausentes SEM usar informaÃ§Ã£o futura\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ”§ Tratando valores ausentes...\")\n",
    "        \n",
    "        print(f\"Valores ausentes ANTES:\")\n",
    "        nan_count = self.df.isnull().sum()\n",
    "        if nan_count.sum() > 0:\n",
    "            print(nan_count[nan_count > 0])\n",
    "        else:\n",
    "            print(\"   âœ… Nenhum\")\n",
    "        \n",
    "        # Forward fill apenas\n",
    "        self.df = self.df.ffill()\n",
    "        \n",
    "        # Remover NaN restantes do inÃ­cio\n",
    "        if self.df.isnull().any().any():\n",
    "            rows_before = len(self.df)\n",
    "            self.df = self.df.dropna()\n",
    "            print(f\"   âœ“ Removidas {rows_before - len(self.df)} linhas iniciais com NaN\")\n",
    "        \n",
    "        print(f\"\\nValores ausentes APÃ“S:\")\n",
    "        nan_count = self.df.isnull().sum()\n",
    "        if nan_count.sum() > 0:\n",
    "            print(nan_count[nan_count > 0])\n",
    "        else:\n",
    "            print(\"   âœ… Nenhum\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def create_technical_indicators(self):\n",
    "        \"\"\"Cria indicadores tÃ©cnicos\"\"\"\n",
    "        print(\"\\nğŸ“ˆ Criando indicadores tÃ©cnicos...\")\n",
    "        \n",
    "        # Moving Averages\n",
    "        self.df['MA_7'] = self.df['Close'].rolling(window=7).mean()\n",
    "        self.df['MA_21'] = self.df['Close'].rolling(window=21).mean()\n",
    "        self.df['MA_50'] = self.df['Close'].rolling(window=50).mean()\n",
    "        \n",
    "        # Exponential Moving Averages\n",
    "        self.df['EMA_12'] = self.df['Close'].ewm(span=12, adjust=False).mean()\n",
    "        self.df['EMA_26'] = self.df['Close'].ewm(span=26, adjust=False).mean()\n",
    "        \n",
    "        # MACD\n",
    "        self.df['MACD'] = self.df['EMA_12'] - self.df['EMA_26']\n",
    "        self.df['MACD_Signal'] = self.df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "        \n",
    "        # RSI\n",
    "        delta = self.df['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        self.df['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        self.df['BB_Middle'] = self.df['Close'].rolling(window=20).mean()\n",
    "        bb_std = self.df['Close'].rolling(window=20).std()\n",
    "        self.df['BB_Upper'] = self.df['BB_Middle'] + (bb_std * 2)\n",
    "        self.df['BB_Lower'] = self.df['BB_Middle'] - (bb_std * 2)\n",
    "        \n",
    "        # Volume\n",
    "        self.df['Volume_MA_7'] = self.df['Volume'].rolling(window=7).mean()\n",
    "        \n",
    "        # Price changes\n",
    "        self.df['Price_Change'] = self.df['Close'].pct_change()\n",
    "        self.df['High_Low_Pct'] = (self.df['High'] - self.df['Low']) / self.df['Close']\n",
    "        \n",
    "        # Volatility\n",
    "        self.df['Volatility'] = self.df['Close'].rolling(window=20).std()\n",
    "        self.df['ATR'] = self.df['High'] - self.df['Low']\n",
    "        \n",
    "        # Remover NaN\n",
    "        rows_before = len(self.df)\n",
    "        self.df = self.df.dropna()\n",
    "        removed = rows_before - len(self.df)\n",
    "        \n",
    "        print(f\"   âœ“ Indicadores criados: {len(self.df.columns)} colunas\")\n",
    "        print(f\"   âœ“ Removidas {removed} linhas (warm-up)\")\n",
    "        print(f\"   âœ“ Registros finais: {len(self.df)}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def select_features(self):\n",
    "        \"\"\"Seleciona features\"\"\"\n",
    "        self.feature_columns = [\n",
    "            'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "            'MA_7', 'MA_21', 'MA_50',\n",
    "            'EMA_12', 'EMA_26',\n",
    "            'MACD', 'MACD_Signal',\n",
    "            'RSI',\n",
    "            'BB_Upper', 'BB_Middle', 'BB_Lower',\n",
    "            'Volume_MA_7',\n",
    "            'Price_Change', 'High_Low_Pct',\n",
    "            'Volatility', 'ATR'\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Features selecionadas: {len(self.feature_columns)}\")\n",
    "        for i, feat in enumerate(self.feature_columns, 1):\n",
    "            print(f\"   {i:2d}. {feat}\")\n",
    "        \n",
    "        return self.feature_columns\n",
    "    \n",
    "    def create_sequences(self, df, seq_length=60, target_column='Close'):\n",
    "        \"\"\"Cria sequÃªncias LSTM\"\"\"\n",
    "        print(f\"\\nğŸ”¢ Criando sequÃªncias...\")\n",
    "        print(f\"   Lookback: {seq_length} timesteps\")\n",
    "        \n",
    "        data = df.values\n",
    "        target_idx = self.feature_columns.index(target_column)\n",
    "        \n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(seq_length, len(data)):\n",
    "            X.append(data[i-seq_length:i])\n",
    "            y.append(data[i, target_idx])\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        print(f\"   âœ“ X shape: {X.shape}\")\n",
    "        print(f\"   âœ“ y shape: {y.shape}\")\n",
    "        print(f\"   âœ“ SequÃªncias: {len(X)}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def split_and_normalize(self, X, y, train_split=0.65, val_split=0.20):\n",
    "        \"\"\"Split temporal + normalizaÃ§Ã£o\"\"\"\n",
    "        print(f\"\\nâœ‚ï¸ Dividindo dados...\")\n",
    "        \n",
    "        train_size = int(len(X) * train_split)\n",
    "        val_size = int(len(X) * val_split)\n",
    "        \n",
    "        X_train = X[:train_size]\n",
    "        y_train = y[:train_size]\n",
    "        \n",
    "        X_val = X[train_size:train_size+val_size]\n",
    "        y_val = y[train_size:train_size+val_size]\n",
    "        \n",
    "        X_test = X[train_size+val_size:]\n",
    "        y_test = y[train_size+val_size:]\n",
    "        \n",
    "        print(f\"   âœ“ Treino: {len(X_train)} ({train_split*100:.0f}%)\")\n",
    "        print(f\"   âœ“ Val: {len(X_val)} ({val_split*100:.0f}%)\")\n",
    "        print(f\"   âœ“ Teste: {len(X_test)} ({(1-train_split-val_split)*100:.0f}%)\")\n",
    "        \n",
    "        # NormalizaÃ§Ã£o\n",
    "        print(f\"\\nğŸ”„ Normalizando...\")\n",
    "        print(f\"   âœ… Fit APENAS no treino\")\n",
    "        \n",
    "        n_samples_train, n_timesteps, n_features = X_train.shape\n",
    "        X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "        \n",
    "        self.scaler.fit(X_train_reshaped)\n",
    "        \n",
    "        X_train_scaled = self.scaler.transform(X_train_reshaped).reshape(n_samples_train, n_timesteps, n_features)\n",
    "        X_val_scaled = self.scaler.transform(X_val.reshape(-1, n_features)).reshape(len(X_val), n_timesteps, n_features)\n",
    "        X_test_scaled = self.scaler.transform(X_test.reshape(-1, n_features)).reshape(len(X_test), n_timesteps, n_features)\n",
    "        \n",
    "        # Normalizar y\n",
    "        close_idx = self.feature_columns.index('Close')\n",
    "        \n",
    "        y_train_full = np.zeros((len(y_train), n_features))\n",
    "        y_val_full = np.zeros((len(y_val), n_features))\n",
    "        y_test_full = np.zeros((len(y_test), n_features))\n",
    "        \n",
    "        y_train_full[:, close_idx] = y_train\n",
    "        y_val_full[:, close_idx] = y_val\n",
    "        y_test_full[:, close_idx] = y_test\n",
    "        \n",
    "        y_train_scaled = self.scaler.transform(y_train_full)[:, close_idx]\n",
    "        y_val_scaled = self.scaler.transform(y_val_full)[:, close_idx]\n",
    "        y_test_scaled = self.scaler.transform(y_test_full)[:, close_idx]\n",
    "        \n",
    "        print(f\"   âœ… NormalizaÃ§Ã£o completa\")\n",
    "        \n",
    "        return (X_train_scaled, y_train_scaled), (X_val_scaled, y_val_scaled), (X_test_scaled, y_test_scaled)\n",
    "    \n",
    "    def save_preprocessor(self, filename='preprocessor.pkl'):\n",
    "        \"\"\"Salva preprocessor\"\"\"\n",
    "        filepath = os.path.join('/kaggle/working/', filename)\n",
    "        \n",
    "        preprocessor_data = {\n",
    "            'scaler': self.scaler,\n",
    "            'feature_columns': self.feature_columns\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(preprocessor_data, f)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Preprocessor salvo: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Pipeline completo\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRÃ‰-PROCESSAMENTO - 100% LIVRE DE DATA LEAKAGE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ConfiguraÃ§Ã£o\n",
    "    INPUT_FILE = 'spy_raw.csv'\n",
    "    \n",
    "    # Carregar\n",
    "    print(f\"\\nğŸ“‚ Carregando: /kaggle/working/{INPUT_FILE}\")\n",
    "    data_path = os.path.join('/kaggle/working/', INPUT_FILE)\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"âŒ Arquivo nÃ£o encontrado: {data_path}\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(data_path, index_col=0, parse_dates=True)\n",
    "    print(f\"   âœ“ Carregados {len(df)} registros\")\n",
    "    \n",
    "    # Preprocessor\n",
    "    preprocessor = StockDataPreprocessor(df)\n",
    "    \n",
    "    # Pipeline\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXECUTANDO PIPELINE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    preprocessor.convert_to_numeric()  # â† NOVA LINHA!\n",
    "    preprocessor.handle_missing_values()\n",
    "    preprocessor.create_technical_indicators()\n",
    "    preprocessor.select_features()\n",
    "    \n",
    "    df_features = preprocessor.df[preprocessor.feature_columns]\n",
    "    \n",
    "    X, y = preprocessor.create_sequences(df_features, seq_length=60)\n",
    "    \n",
    "    train_data, val_data, test_data = preprocessor.split_and_normalize(\n",
    "        X, y, train_split=0.65, val_split=0.20\n",
    "    )\n",
    "    \n",
    "    # Salvar\n",
    "    print(f\"\\nğŸ’¾ Salvando dados...\")\n",
    "    np.save('/kaggle/working/X_train.npy', train_data[0])\n",
    "    np.save('/kaggle/working/y_train.npy', train_data[1])\n",
    "    np.save('/kaggle/working/X_val.npy', val_data[0])\n",
    "    np.save('/kaggle/working/y_val.npy', val_data[1])\n",
    "    np.save('/kaggle/working/X_test.npy', test_data[0])\n",
    "    np.save('/kaggle/working/y_test.npy', test_data[1])\n",
    "    print(f\"   âœ“ 6 arquivos .npy salvos\")\n",
    "    \n",
    "    preprocessor.save_preprocessor()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"âœ… PRÃ‰-PROCESSAMENTO CONCLUÃDO\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nğŸ“‹ Checklist:\")\n",
    "    print(f\"   âœ… ConversÃ£o para numÃ©rico\")\n",
    "    print(f\"   âœ… Missing values (forward fill)\")\n",
    "    print(f\"   âœ… Indicadores (janela passada)\")\n",
    "    print(f\"   âœ… SequÃªncias (sem leakage)\")\n",
    "    print(f\"   âœ… Split (temporal)\")\n",
    "    print(f\"   âœ… NormalizaÃ§Ã£o (fit no treino)\")\n",
    "    \n",
    "    return preprocessor, train_data, val_data, test_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "855966a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T17:40:41.736224Z",
     "iopub.status.busy": "2026-02-20T17:40:41.735687Z",
     "iopub.status.idle": "2026-02-20T17:40:41.740350Z",
     "shell.execute_reply": "2026-02-20T17:40:41.739693Z"
    },
    "papermill": {
     "duration": 0.009274,
     "end_time": "2026-02-20T17:40:41.741594",
     "exception": false,
     "start_time": "2026-02-20T17:40:41.732320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "GPU Devices:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "âœ… GPU detectada! TensorFlow usarÃ¡ GPU automaticamente.\n"
     ]
    }
   ],
   "source": [
    "# Verificar GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Se quiser forÃ§ar uso de GPU\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"âœ… GPU detectada! TensorFlow usarÃ¡ GPU automaticamente.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Nenhuma GPU detectada. Usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f120b7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T17:40:41.747642Z",
     "iopub.status.busy": "2026-02-20T17:40:41.747410Z",
     "iopub.status.idle": "2026-02-20T17:41:47.834755Z",
     "shell.execute_reply": "2026-02-20T17:41:47.833860Z"
    },
    "papermill": {
     "duration": 66.092406,
     "end_time": "2026-02-20T17:41:47.836360",
     "exception": false,
     "start_time": "2026-02-20T17:40:41.743954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELAGEM LSTM\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‚ Carregando dados prÃ©-processados...\n",
      "   âœ“ Dados carregados!\n",
      "   âœ“ X_train: (2566, 60, 21)\n",
      "   âœ“ X_val: (789, 60, 21)\n",
      "   âœ“ X_test: (593, 60, 21)\n",
      "\n",
      "ğŸ—ï¸ Construindo modelo LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1771609242.381103      24 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13757 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1771609242.386904      24 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13757 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Modelo construÃ­do!\n",
      "\n",
      "ğŸ“‹ Arquitetura:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">76,800</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">825</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚        \u001b[38;5;34m76,800\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚        \u001b[38;5;34m49,408\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m12,416\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             â”‚           \u001b[38;5;34m825\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m26\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139,475</span> (544.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m139,475\u001b[0m (544.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139,475</span> (544.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m139,475\u001b[0m (544.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Iniciando treinamento...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1771609247.785662      72 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0698 - mae: 0.1755\n",
      "Epoch 1: val_loss improved from inf to 0.10310, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - loss: 0.0693 - mae: 0.1748 - val_loss: 0.1031 - val_mae: 0.3003 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0110 - mae: 0.0743\n",
      "Epoch 2: val_loss improved from 0.10310 to 0.07107, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0110 - mae: 0.0743 - val_loss: 0.0711 - val_mae: 0.2431 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0096 - mae: 0.0695\n",
      "Epoch 3: val_loss improved from 0.07107 to 0.05684, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0096 - mae: 0.0694 - val_loss: 0.0568 - val_mae: 0.2180 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0099 - mae: 0.0671\n",
      "Epoch 4: val_loss improved from 0.05684 to 0.05526, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0098 - mae: 0.0670 - val_loss: 0.0553 - val_mae: 0.2181 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0074 - mae: 0.0584\n",
      "Epoch 5: val_loss improved from 0.05526 to 0.05264, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0074 - mae: 0.0583 - val_loss: 0.0526 - val_mae: 0.2156 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0064 - mae: 0.0547\n",
      "Epoch 6: val_loss improved from 0.05264 to 0.03467, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0063 - mae: 0.0547 - val_loss: 0.0347 - val_mae: 0.1714 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m80/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0056 - mae: 0.0524\n",
      "Epoch 7: val_loss improved from 0.03467 to 0.02607, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0056 - mae: 0.0523 - val_loss: 0.0261 - val_mae: 0.1469 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m78/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0052 - mae: 0.0504\n",
      "Epoch 8: val_loss improved from 0.02607 to 0.01943, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0052 - mae: 0.0504 - val_loss: 0.0194 - val_mae: 0.1236 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0056 - mae: 0.0517\n",
      "Epoch 9: val_loss improved from 0.01943 to 0.01664, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0056 - mae: 0.0517 - val_loss: 0.0166 - val_mae: 0.1127 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0050 - mae: 0.0503\n",
      "Epoch 10: val_loss improved from 0.01664 to 0.01643, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0050 - mae: 0.0502 - val_loss: 0.0164 - val_mae: 0.1130 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0047 - mae: 0.0485\n",
      "Epoch 11: val_loss improved from 0.01643 to 0.01581, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0047 - mae: 0.0485 - val_loss: 0.0158 - val_mae: 0.1100 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0039 - mae: 0.0450\n",
      "Epoch 12: val_loss improved from 0.01581 to 0.01022, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0039 - mae: 0.0450 - val_loss: 0.0102 - val_mae: 0.0848 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0042 - mae: 0.0450\n",
      "Epoch 13: val_loss did not improve from 0.01022\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0042 - mae: 0.0450 - val_loss: 0.0307 - val_mae: 0.1629 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0040 - mae: 0.0448\n",
      "Epoch 14: val_loss did not improve from 0.01022\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0040 - mae: 0.0448 - val_loss: 0.0172 - val_mae: 0.1172 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0039 - mae: 0.0441\n",
      "Epoch 15: val_loss did not improve from 0.01022\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0039 - mae: 0.0441 - val_loss: 0.0178 - val_mae: 0.1204 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0036 - mae: 0.0437\n",
      "Epoch 16: val_loss did not improve from 0.01022\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0036 - mae: 0.0437 - val_loss: 0.0332 - val_mae: 0.1705 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0035 - mae: 0.0425\n",
      "Epoch 17: val_loss improved from 0.01022 to 0.00875, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0035 - mae: 0.0425 - val_loss: 0.0087 - val_mae: 0.0790 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0036 - mae: 0.0423\n",
      "Epoch 18: val_loss improved from 0.00875 to 0.00696, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0036 - mae: 0.0423 - val_loss: 0.0070 - val_mae: 0.0694 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0035 - mae: 0.0434\n",
      "Epoch 19: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0035 - mae: 0.0434 - val_loss: 0.0188 - val_mae: 0.1235 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0033 - mae: 0.0418\n",
      "Epoch 20: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0033 - mae: 0.0418 - val_loss: 0.0447 - val_mae: 0.2013 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0029 - mae: 0.0390\n",
      "Epoch 21: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0029 - mae: 0.0391 - val_loss: 0.0233 - val_mae: 0.1407 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0032 - mae: 0.0410\n",
      "Epoch 22: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0032 - mae: 0.0410 - val_loss: 0.0206 - val_mae: 0.1311 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0030 - mae: 0.0397\n",
      "Epoch 23: val_loss did not improve from 0.00696\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0030 - mae: 0.0397 - val_loss: 0.0186 - val_mae: 0.1247 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0030 - mae: 0.0392\n",
      "Epoch 24: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0030 - mae: 0.0392 - val_loss: 0.0170 - val_mae: 0.1188 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m80/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0031 - mae: 0.0390\n",
      "Epoch 25: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0031 - mae: 0.0390 - val_loss: 0.0207 - val_mae: 0.1328 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025 - mae: 0.0358\n",
      "Epoch 26: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0025 - mae: 0.0359 - val_loss: 0.0180 - val_mae: 0.1223 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0027 - mae: 0.0372\n",
      "Epoch 27: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0027 - mae: 0.0372 - val_loss: 0.0080 - val_mae: 0.0759 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - mae: 0.0370\n",
      "Epoch 28: val_loss did not improve from 0.00696\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0026 - mae: 0.0370 - val_loss: 0.0237 - val_mae: 0.1435 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0028 - mae: 0.0376\n",
      "Epoch 29: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0088 - val_mae: 0.0810 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - mae: 0.0359\n",
      "Epoch 30: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0026 - mae: 0.0359 - val_loss: 0.0083 - val_mae: 0.0779 - learning_rate: 2.5000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024 - mae: 0.0353\n",
      "Epoch 31: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0024 - mae: 0.0353 - val_loss: 0.0167 - val_mae: 0.1170 - learning_rate: 2.5000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025 - mae: 0.0364\n",
      "Epoch 32: val_loss did not improve from 0.00696\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0025 - mae: 0.0364 - val_loss: 0.0083 - val_mae: 0.0773 - learning_rate: 2.5000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - mae: 0.0367\n",
      "Epoch 33: val_loss improved from 0.00696 to 0.00560, saving model to /kaggle/working/best_model.keras\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0026 - mae: 0.0367 - val_loss: 0.0056 - val_mae: 0.0616 - learning_rate: 2.5000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m80/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0028 - mae: 0.0375\n",
      "Epoch 34: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0088 - val_mae: 0.0808 - learning_rate: 2.5000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025 - mae: 0.0360\n",
      "Epoch 35: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0025 - mae: 0.0360 - val_loss: 0.0083 - val_mae: 0.0780 - learning_rate: 2.5000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0027 - mae: 0.0373\n",
      "Epoch 36: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0027 - mae: 0.0373 - val_loss: 0.0164 - val_mae: 0.1163 - learning_rate: 2.5000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0028 - mae: 0.0388\n",
      "Epoch 37: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0028 - mae: 0.0388 - val_loss: 0.0163 - val_mae: 0.1160 - learning_rate: 2.5000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - mae: 0.0368\n",
      "Epoch 38: val_loss did not improve from 0.00560\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0026 - mae: 0.0368 - val_loss: 0.0074 - val_mae: 0.0733 - learning_rate: 2.5000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024 - mae: 0.0354\n",
      "Epoch 39: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0024 - mae: 0.0354 - val_loss: 0.0073 - val_mae: 0.0729 - learning_rate: 1.2500e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - mae: 0.0347\n",
      "Epoch 40: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0023 - mae: 0.0347 - val_loss: 0.0070 - val_mae: 0.0707 - learning_rate: 1.2500e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024 - mae: 0.0356\n",
      "Epoch 41: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0024 - mae: 0.0356 - val_loss: 0.0071 - val_mae: 0.0722 - learning_rate: 1.2500e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - mae: 0.0349\n",
      "Epoch 42: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0023 - mae: 0.0349 - val_loss: 0.0105 - val_mae: 0.0907 - learning_rate: 1.2500e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m80/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - mae: 0.0355\n",
      "Epoch 43: val_loss did not improve from 0.00560\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0024 - mae: 0.0355 - val_loss: 0.0062 - val_mae: 0.0659 - learning_rate: 1.2500e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025 - mae: 0.0358\n",
      "Epoch 44: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0025 - mae: 0.0358 - val_loss: 0.0071 - val_mae: 0.0718 - learning_rate: 6.2500e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - mae: 0.0348\n",
      "Epoch 45: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0023 - mae: 0.0348 - val_loss: 0.0059 - val_mae: 0.0641 - learning_rate: 6.2500e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m80/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - mae: 0.0367\n",
      "Epoch 46: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0026 - mae: 0.0366 - val_loss: 0.0108 - val_mae: 0.0922 - learning_rate: 6.2500e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024 - mae: 0.0345\n",
      "Epoch 47: val_loss did not improve from 0.00560\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0024 - mae: 0.0345 - val_loss: 0.0060 - val_mae: 0.0647 - learning_rate: 6.2500e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0027 - mae: 0.0369\n",
      "Epoch 48: val_loss did not improve from 0.00560\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0027 - mae: 0.0369 - val_loss: 0.0098 - val_mae: 0.0867 - learning_rate: 6.2500e-05\n",
      "Epoch 48: early stopping\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "\n",
      "âœ… Treinamento concluÃ­do!\n",
      "ğŸ’¾ GrÃ¡fico salvo: /kaggle/working/training_history.png\n",
      "\n",
      "ğŸ“Š Avaliando modelo...\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\n",
      "ğŸ“ˆ MÃ‰TRICAS DE AVALIAÃ‡ÃƒO:\n",
      "   MAE (Mean Absolute Error): $101.3182\n",
      "   RMSE (Root Mean Square Error): $113.4031\n",
      "   MAPE (Mean Absolute Percentage Error): 17.02%\n",
      "ğŸ’¾ GrÃ¡fico salvo: /kaggle/working/predictions_vs_real.png\n",
      "ğŸ’¾ GrÃ¡fico salvo: /kaggle/working/error_analysis.png\n",
      "\n",
      "ğŸ’¾ Modelo salvo: /kaggle/working/lstm_stock_model.keras\n",
      "ğŸ’¾ MÃ©tricas salvas: /kaggle/working/model_metrics.csv\n",
      "\n",
      "================================================================================\n",
      "âœ… MODELAGEM COMPLETA!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Arquivos gerados:\n",
      "   â€¢ lstm_stock_model.keras\n",
      "   â€¢ best_model.keras\n",
      "   â€¢ training_history.png\n",
      "   â€¢ predictions_vs_real.png\n",
      "   â€¢ error_analysis.png\n",
      "   â€¢ model_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "========================================\n",
    "SCRIPT 3: MODELAGEM LSTM\n",
    "VersÃ£o Final - Treinamento e AvaliaÃ§Ã£o\n",
    "========================================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Seed para reprodutibilidade\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "class LSTMStockPredictor:\n",
    "    \"\"\"Modelo LSTM para prediÃ§Ã£o de aÃ§Ãµes\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def build_model(self, lstm_units=[128, 64, 32], dropout_rate=0.2, learning_rate=0.001):\n",
    "        \"\"\"ConstrÃ³i arquitetura LSTM\"\"\"\n",
    "        print(\"\\nğŸ—ï¸ Construindo modelo LSTM...\")\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Primeira LSTM (retorna sequÃªncias)\n",
    "        model.add(LSTM(units=lstm_units[0], return_sequences=True, input_shape=self.input_shape))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        # LSTMs intermediÃ¡rias\n",
    "        for units in lstm_units[1:-1]:\n",
    "            model.add(LSTM(units=units, return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        # Ãšltima LSTM (nÃ£o retorna sequÃªncias)\n",
    "        model.add(LSTM(units=lstm_units[-1], return_sequences=False))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        # Dense layers\n",
    "        model.add(Dense(units=25, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(units=1))\n",
    "        \n",
    "        # Compilar\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(f\"   âœ“ Modelo construÃ­do!\")\n",
    "        print(f\"\\nğŸ“‹ Arquitetura:\")\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32, verbose=1):\n",
    "        \"\"\"Treina o modelo\"\"\"\n",
    "        print(\"\\nğŸ“ Iniciando treinamento...\")\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            '/kaggle/working/best_model.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Treinar\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        self.history = history\n",
    "        print(\"\\nâœ… Treinamento concluÃ­do!\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, scaler, feature_columns):\n",
    "        \"\"\"Avalia o modelo e calcula mÃ©tricas\"\"\"\n",
    "        print(\"\\nğŸ“Š Avaliando modelo...\")\n",
    "        \n",
    "        # PrediÃ§Ãµes\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Desnormalizar\n",
    "        close_idx = feature_columns.index('Close')\n",
    "        \n",
    "        y_test_full = np.zeros((len(y_test), len(feature_columns)))\n",
    "        y_pred_full = np.zeros((len(y_pred), len(feature_columns)))\n",
    "        \n",
    "        y_test_full[:, close_idx] = y_test\n",
    "        y_pred_full[:, close_idx] = y_pred.flatten()\n",
    "        \n",
    "        y_test_original = scaler.inverse_transform(y_test_full)[:, close_idx]\n",
    "        y_pred_original = scaler.inverse_transform(y_pred_full)[:, close_idx]\n",
    "        \n",
    "        # MÃ©tricas\n",
    "        mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "        mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ MÃ‰TRICAS DE AVALIAÃ‡ÃƒO:\")\n",
    "        print(f\"   MAE (Mean Absolute Error): ${mae:.4f}\")\n",
    "        print(f\"   RMSE (Root Mean Square Error): ${rmse:.4f}\")\n",
    "        print(f\"   MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            'y_test': y_test_original,\n",
    "            'y_pred': y_pred_original,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape\n",
    "        }\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plota histÃ³rico de treinamento\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"âš ï¸ Modelo ainda nÃ£o foi treinado!\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss\n",
    "        ax1.plot(self.history.history['loss'], label='Treino')\n",
    "        ax1.plot(self.history.history['val_loss'], label='ValidaÃ§Ã£o')\n",
    "        ax1.set_title('Loss durante Treinamento')\n",
    "        ax1.set_xlabel('Ã‰poca')\n",
    "        ax1.set_ylabel('Loss (MSE)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # MAE\n",
    "        ax2.plot(self.history.history['mae'], label='Treino')\n",
    "        ax2.plot(self.history.history['val_mae'], label='ValidaÃ§Ã£o')\n",
    "        ax2.set_title('MAE durante Treinamento')\n",
    "        ax2.set_xlabel('Ã‰poca')\n",
    "        ax2.set_ylabel('MAE')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/kaggle/working/training_history.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ’¾ GrÃ¡fico salvo: /kaggle/working/training_history.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_predictions(self, evaluation_results, num_points=500):\n",
    "        \"\"\"Plota prediÃ§Ãµes vs valores reais\"\"\"\n",
    "        y_test = evaluation_results['y_test'][-num_points:]\n",
    "        y_pred = evaluation_results['y_pred'][-num_points:]\n",
    "        \n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(y_test, label='Valor Real', linewidth=2, alpha=0.8)\n",
    "        plt.plot(y_pred, label='PrediÃ§Ã£o', linewidth=2, alpha=0.8)\n",
    "        plt.title(f'PrediÃ§Ã£o vs Valor Real (Ãºltimos {num_points} pontos)', fontsize=14)\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('PreÃ§o de Fechamento ($)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/kaggle/working/predictions_vs_real.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ’¾ GrÃ¡fico salvo: /kaggle/working/predictions_vs_real.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_error_distribution(self, evaluation_results):\n",
    "        \"\"\"Plota distribuiÃ§Ã£o dos erros\"\"\"\n",
    "        y_test = evaluation_results['y_test']\n",
    "        y_pred = evaluation_results['y_pred']\n",
    "        errors = y_test - y_pred\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Histograma\n",
    "        ax1.hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "        ax1.set_title('DistribuiÃ§Ã£o dos Erros')\n",
    "        ax1.set_xlabel('Erro ($)')\n",
    "        ax1.set_ylabel('FrequÃªncia')\n",
    "        ax1.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Scatter\n",
    "        ax2.scatter(y_test, y_pred, alpha=0.5)\n",
    "        ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', linewidth=2, label='PrediÃ§Ã£o Perfeita')\n",
    "        ax2.set_title('PrediÃ§Ã£o vs Valor Real')\n",
    "        ax2.set_xlabel('Valor Real ($)')\n",
    "        ax2.set_ylabel('PrediÃ§Ã£o ($)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/kaggle/working/error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ’¾ GrÃ¡fico salvo: /kaggle/working/error_analysis.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def save_model(self, filename='lstm_model.keras'):\n",
    "        \"\"\"Salva modelo\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"âš ï¸ Nenhum modelo para salvar!\")\n",
    "            return\n",
    "        \n",
    "        filepath = os.path.join('/kaggle/working/', filename)\n",
    "        self.model.save(filepath)\n",
    "        print(f\"\\nğŸ’¾ Modelo salvo: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Executa treinamento completo\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODELAGEM LSTM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Carregar dados\n",
    "    print(\"\\nğŸ“‚ Carregando dados prÃ©-processados...\")\n",
    "    \n",
    "    try:\n",
    "        X_train = np.load('/kaggle/working/X_train.npy')\n",
    "        y_train = np.load('/kaggle/working/y_train.npy')\n",
    "        X_val = np.load('/kaggle/working/X_val.npy')\n",
    "        y_val = np.load('/kaggle/working/y_val.npy')\n",
    "        X_test = np.load('/kaggle/working/X_test.npy')\n",
    "        y_test = np.load('/kaggle/working/y_test.npy')\n",
    "        \n",
    "        with open('/kaggle/working/preprocessor.pkl', 'rb') as f:\n",
    "            preprocessor_data = pickle.load(f)\n",
    "        \n",
    "        scaler = preprocessor_data['scaler']\n",
    "        feature_columns = preprocessor_data['feature_columns']\n",
    "        \n",
    "        print(f\"   âœ“ Dados carregados!\")\n",
    "        print(f\"   âœ“ X_train: {X_train.shape}\")\n",
    "        print(f\"   âœ“ X_val: {X_val.shape}\")\n",
    "        print(f\"   âœ“ X_test: {X_test.shape}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ Erro: {e}\")\n",
    "        print(\"   Execute o script de prÃ©-processamento primeiro!\")\n",
    "        return None\n",
    "    \n",
    "    # Criar modelo\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    predictor = LSTMStockPredictor(input_shape)\n",
    "    \n",
    "    # Construir\n",
    "    predictor.build_model(\n",
    "        lstm_units=[128, 64, 32],\n",
    "        dropout_rate=0.2,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    # Treinar\n",
    "    history = predictor.train(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Visualizar histÃ³rico\n",
    "    predictor.plot_training_history()\n",
    "    \n",
    "    # Avaliar\n",
    "    evaluation_results = predictor.evaluate(X_test, y_test, scaler, feature_columns)\n",
    "    \n",
    "    # Visualizar prediÃ§Ãµes\n",
    "    predictor.plot_predictions(evaluation_results, num_points=500)\n",
    "    predictor.plot_error_distribution(evaluation_results)\n",
    "    \n",
    "    # Salvar modelo\n",
    "    predictor.save_model('lstm_stock_model.keras')\n",
    "    \n",
    "    # Salvar mÃ©tricas\n",
    "    metrics_df = pd.DataFrame([{\n",
    "        'MAE': evaluation_results['mae'],\n",
    "        'RMSE': evaluation_results['rmse'],\n",
    "        'MAPE': evaluation_results['mape'],\n",
    "        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }])\n",
    "    metrics_df.to_csv('/kaggle/working/model_metrics.csv', index=False)\n",
    "    print(f\"ğŸ’¾ MÃ©tricas salvas: /kaggle/working/model_metrics.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… MODELAGEM COMPLETA!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nğŸ“ Arquivos gerados:\")\n",
    "    print(f\"   â€¢ lstm_stock_model.keras\")\n",
    "    print(f\"   â€¢ best_model.keras\")\n",
    "    print(f\"   â€¢ training_history.png\")\n",
    "    print(f\"   â€¢ predictions_vs_real.png\")\n",
    "    print(f\"   â€¢ error_analysis.png\")\n",
    "    print(f\"   â€¢ model_metrics.csv\")\n",
    "    \n",
    "    return predictor, evaluation_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f6663",
   "metadata": {
    "papermill": {
     "duration": 0.037572,
     "end_time": "2026-02-20T17:41:47.913660",
     "exception": false,
     "start_time": "2026-02-20T17:41:47.876088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 101.807584,
   "end_time": "2026-02-20T17:41:51.388704",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-20T17:40:09.581120",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
