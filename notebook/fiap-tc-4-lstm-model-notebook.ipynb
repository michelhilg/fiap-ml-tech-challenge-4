{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94bfb095",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-10T02:37:16.046834Z",
     "iopub.status.busy": "2026-02-10T02:37:16.046527Z",
     "iopub.status.idle": "2026-02-10T02:37:18.300186Z",
     "shell.execute_reply": "2026-02-10T02:37:18.299365Z"
    },
    "papermill": {
     "duration": 2.260204,
     "end_time": "2026-02-10T02:37:18.301816",
     "exception": false,
     "start_time": "2026-02-10T02:37:16.041612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COLETA DE DADOS - VERS√ÉO SIMPLIFICADA\n",
      "================================================================================\n",
      "\n",
      "Configura√ß√£o:\n",
      "   Ticker: SPY\n",
      "   Data inicial: 2010-01-01\n",
      "   Intervalo: Di√°rio (1d)\n",
      "\n",
      "================================================================================\n",
      "COLETANDO DADOS: SPY\n",
      "================================================================================\n",
      "üìä Intervalo: DI√ÅRIO (1d)\n",
      "üìÖ Per√≠odo: 2010-01-01 at√© hoje\n",
      "\n",
      "‚úÖ Coleta bem-sucedida!\n",
      "   üìä Total de dias: 4050\n",
      "   üìÖ Per√≠odo: 2010-01-04 at√© 2026-02-09\n",
      "   üìà Colunas: [('Close', 'SPY'), ('High', 'SPY'), ('Low', 'SPY'), ('Open', 'SPY'), ('Volume', 'SPY')]\n",
      "\n",
      "================================================================================\n",
      "INFORMA√á√ïES DO DATASET: SPY\n",
      "================================================================================\n",
      "\n",
      "üìä Dimens√µes: 4050 linhas √ó 5 colunas\n",
      "üìÖ Per√≠odo: 2010-01-04 at√© 2026-02-09\n",
      "‚è∞ Dura√ß√£o: 5880 dias\n",
      "\n",
      "üìà Primeiras 3 linhas:\n",
      "Price           Close       High        Low       Open     Volume\n",
      "Ticker            SPY        SPY        SPY        SPY        SPY\n",
      "Date                                                             \n",
      "2010-01-04  85.027992  85.073007  83.662503  84.307735  118944600\n",
      "2010-01-05  85.253052  85.290567  84.667843  84.975456  111579900\n",
      "2010-01-06  85.313080  85.523154  85.103005  85.170527  116074400\n",
      "\n",
      "üìà √öltimas 3 linhas:\n",
      "Price            Close        High         Low        Open     Volume\n",
      "Ticker             SPY         SPY         SPY         SPY        SPY\n",
      "Date                                                                 \n",
      "2026-02-05  677.619995  683.690002  675.789978  680.940002  113610800\n",
      "2026-02-06  690.619995  692.309998  680.849976  681.460022   89127600\n",
      "2026-02-09  693.950012  695.869995  688.340027  689.419983   73570500\n",
      "\n",
      "üìä Estat√≠sticas:\n",
      "Price         Close         High          Low         Open        Volume\n",
      "Ticker          SPY          SPY          SPY          SPY           SPY\n",
      "count   4050.000000  4050.000000  4050.000000  4050.000000  4.050000e+03\n",
      "mean     271.507052   272.909178   269.871188   271.457179  1.091936e+08\n",
      "std      159.119664   159.892981   158.198689   159.096337  6.779471e+07\n",
      "min       77.359520    78.282991    76.549627    78.048340  2.027000e+07\n",
      "25%      148.043255   148.635806   147.124954   147.446653  6.524735e+07\n",
      "50%      231.085838   233.154377   228.539625   231.120345  8.933345e+07\n",
      "75%      389.845932   391.698192   387.083443   389.509499  1.324971e+08\n",
      "max      695.489990   697.840027   693.940002   697.049988  7.178287e+08\n",
      "\n",
      "‚ö†Ô∏è Valores ausentes:\n",
      "   ‚úÖ Nenhum valor ausente\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üíæ Dados salvos: /kaggle/working/spy_raw.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ COLETA CONCLU√çDA COM SUCESSO!\n",
      "================================================================================\n",
      "üìÅ Arquivo salvo: /kaggle/working/spy_raw.csv\n",
      "üìä Total de linhas: 4050\n",
      "\n",
      "‚û°Ô∏è Pr√≥ximo passo: Execute o script de pr√©-processamento\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "========================================\n",
    "SCRIPT 1: COLETA DE DADOS\n",
    "Vers√£o Simplificada - APENAS dados DI√ÅRIOS\n",
    "========================================\n",
    "\"\"\"\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class StockDataCollector:\n",
    "    \"\"\"Coletor simplificado - apenas dados di√°rios\"\"\"\n",
    "    \n",
    "    def __init__(self, symbol='SPY'):\n",
    "        self.symbol = symbol\n",
    "        self.df = None\n",
    "        \n",
    "    def collect_data(self, start_date='2010-01-01'):\n",
    "        \"\"\"\n",
    "        Coleta dados di√°rios desde start_date at√© hoje\n",
    "        \n",
    "        Par√¢metros:\n",
    "        - start_date: Data inicial (padr√£o: 2010-01-01)\n",
    "                     Use '2000-01-01' para mais dados\n",
    "                     Use '2015-01-01' para menos dados\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"COLETANDO DADOS: {self.symbol}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"üìä Intervalo: DI√ÅRIO (1d)\")\n",
    "        print(f\"üìÖ Per√≠odo: {start_date} at√© hoje\")\n",
    "        \n",
    "        # Coletar dados\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        self.df = yf.download(\n",
    "            self.symbol,\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "            interval='1d',\n",
    "            progress=False,\n",
    "            auto_adjust=True  # Ajusta automaticamente por splits/dividendos\n",
    "        )\n",
    "        \n",
    "        # Verificar se conseguiu dados\n",
    "        if len(self.df) == 0:\n",
    "            print(f\"\\n‚ùå ERRO: Nenhum dado coletado para {self.symbol}\")\n",
    "            print(f\"   Verifique se o ticker est√° correto\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n‚úÖ Coleta bem-sucedida!\")\n",
    "        print(f\"   üìä Total de dias: {len(self.df)}\")\n",
    "        print(f\"   üìÖ Per√≠odo: {self.df.index[0].date()} at√© {self.df.index[-1].date()}\")\n",
    "        print(f\"   üìà Colunas: {list(self.df.columns)}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def save_data(self, filename=None):\n",
    "        \"\"\"Salva dados coletados\"\"\"\n",
    "        if self.df is None:\n",
    "            print(\"‚ö†Ô∏è Nenhum dado para salvar. Execute collect_data() primeiro.\")\n",
    "            return None\n",
    "        \n",
    "        if filename is None:\n",
    "            filename = f'{self.symbol.lower()}_raw.csv'\n",
    "        \n",
    "        filepath = os.path.join('/kaggle/working/', filename)\n",
    "        self.df.to_csv(filepath)\n",
    "        \n",
    "        print(f\"\\nüíæ Dados salvos: {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Mostra informa√ß√µes detalhadas dos dados\"\"\"\n",
    "        if self.df is None:\n",
    "            print(\"‚ö†Ô∏è Nenhum dado dispon√≠vel\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"INFORMA√á√ïES DO DATASET: {self.symbol}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nüìä Dimens√µes: {self.df.shape[0]} linhas √ó {self.df.shape[1]} colunas\")\n",
    "        print(f\"üìÖ Per√≠odo: {self.df.index[0].date()} at√© {self.df.index[-1].date()}\")\n",
    "        print(f\"‚è∞ Dura√ß√£o: {(self.df.index[-1] - self.df.index[0]).days} dias\")\n",
    "        \n",
    "        print(f\"\\nüìà Primeiras 3 linhas:\")\n",
    "        print(self.df.head(3))\n",
    "        \n",
    "        print(f\"\\nüìà √öltimas 3 linhas:\")\n",
    "        print(self.df.tail(3))\n",
    "        \n",
    "        print(f\"\\nüìä Estat√≠sticas:\")\n",
    "        print(self.df.describe())\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è Valores ausentes:\")\n",
    "        missing = self.df.isnull().sum()\n",
    "        if missing.sum() > 0:\n",
    "            print(missing[missing > 0])\n",
    "        else:\n",
    "            print(\"   ‚úÖ Nenhum valor ausente\")\n",
    "        \n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fun√ß√£o principal - executa coleta completa\"\"\"\n",
    "    \n",
    "    # Configura√ß√£o\n",
    "    SYMBOL = 'SPY'  # ‚Üê MUDE AQUI: 'SPY', 'GOLD', 'MSFT', 'AAPL', etc.\n",
    "    START_DATE = '2010-01-01'  # ‚Üê MUDE AQUI para mais/menos dados\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COLETA DE DADOS - VERS√ÉO SIMPLIFICADA\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nConfigura√ß√£o:\")\n",
    "    print(f\"   Ticker: {SYMBOL}\")\n",
    "    print(f\"   Data inicial: {START_DATE}\")\n",
    "    print(f\"   Intervalo: Di√°rio (1d)\")\n",
    "    \n",
    "    # Criar coletor\n",
    "    collector = StockDataCollector(symbol=SYMBOL)\n",
    "    \n",
    "    # Coletar dados\n",
    "    df = collector.collect_data(start_date=START_DATE)\n",
    "    \n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Mostrar informa√ß√µes\n",
    "    collector.get_info()\n",
    "    \n",
    "    # Salvar\n",
    "    filepath = collector.save_data()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ COLETA CONCLU√çDA COM SUCESSO!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÅ Arquivo salvo: {filepath}\")\n",
    "    print(f\"üìä Total de linhas: {len(df)}\")\n",
    "    print(f\"\\n‚û°Ô∏è Pr√≥ximo passo: Execute o script de pr√©-processamento\")\n",
    "    \n",
    "    return collector\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collector = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609d0628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T02:37:18.307680Z",
     "iopub.status.busy": "2026-02-10T02:37:18.307430Z",
     "iopub.status.idle": "2026-02-10T02:37:18.315584Z",
     "shell.execute_reply": "2026-02-10T02:37:18.314787Z"
    },
    "papermill": {
     "duration": 0.012413,
     "end_time": "2026-02-10T02:37:18.316734",
     "exception": true,
     "start_time": "2026-02-10T02:37:18.304321",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25/921338735.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Verificar GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Num GPUs Available: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU Devices: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Se quiser for√ßar uso de GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Verificar GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Se quiser for√ßar uso de GPU\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"‚úÖ GPU detectada! TensorFlow usar√° GPU automaticamente.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhuma GPU detectada. Usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041dab88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T05:39:44.069442Z",
     "iopub.status.busy": "2026-02-09T05:39:44.069134Z",
     "iopub.status.idle": "2026-02-09T05:39:44.218100Z",
     "shell.execute_reply": "2026-02-09T05:39:44.217413Z",
     "shell.execute_reply.started": "2026-02-09T05:39:44.069406Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "========================================\n",
    "SCRIPT 2: PR√â-PROCESSAMENTO (CORRIGIDO)\n",
    "Vers√£o Limpa - SEM data leakage\n",
    "+ Convers√£o de tipos\n",
    "========================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "class StockDataPreprocessor:\n",
    "    \"\"\"Pr√©-processador limpo e auditado\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.feature_columns = []\n",
    "    \n",
    "    def convert_to_numeric(self):\n",
    "        \"\"\"\n",
    "        ‚úÖ Converte todas as colunas para num√©rico\n",
    "        Resolve problema de dados lidos como string\n",
    "        \"\"\"\n",
    "        print(\"\\nüîÑ Convertendo colunas para num√©rico...\")\n",
    "        \n",
    "        numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in self.df.columns:\n",
    "                # Converter para num√©rico, erros viram NaN\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "        \n",
    "        # Verificar tipos\n",
    "        print(f\"   ‚úì Tipos ap√≥s convers√£o:\")\n",
    "        for col in numeric_columns:\n",
    "            if col in self.df.columns:\n",
    "                print(f\"      {col}: {self.df[col].dtype}\")\n",
    "        \n",
    "        # Verificar NaN ap√≥s convers√£o\n",
    "        nan_count = self.df[numeric_columns].isnull().sum()\n",
    "        if nan_count.sum() > 0:\n",
    "            print(f\"\\n   ‚ö†Ô∏è NaNs ap√≥s convers√£o (valores n√£o-num√©ricos):\")\n",
    "            print(nan_count[nan_count > 0])\n",
    "        else:\n",
    "            print(f\"   ‚úì Sem NaNs ap√≥s convers√£o\")\n",
    "        \n",
    "        return self.df\n",
    "        \n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"\n",
    "        ‚úÖ Trata valores ausentes SEM usar informa√ß√£o futura\n",
    "        \"\"\"\n",
    "        print(\"\\nüîß Tratando valores ausentes...\")\n",
    "        \n",
    "        print(f\"Valores ausentes ANTES:\")\n",
    "        nan_count = self.df.isnull().sum()\n",
    "        if nan_count.sum() > 0:\n",
    "            print(nan_count[nan_count > 0])\n",
    "        else:\n",
    "            print(\"   ‚úÖ Nenhum\")\n",
    "        \n",
    "        # Forward fill apenas\n",
    "        self.df = self.df.ffill()\n",
    "        \n",
    "        # Remover NaN restantes do in√≠cio\n",
    "        if self.df.isnull().any().any():\n",
    "            rows_before = len(self.df)\n",
    "            self.df = self.df.dropna()\n",
    "            print(f\"   ‚úì Removidas {rows_before - len(self.df)} linhas iniciais com NaN\")\n",
    "        \n",
    "        print(f\"\\nValores ausentes AP√ìS:\")\n",
    "        nan_count = self.df.isnull().sum()\n",
    "        if nan_count.sum() > 0:\n",
    "            print(nan_count[nan_count > 0])\n",
    "        else:\n",
    "            print(\"   ‚úÖ Nenhum\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def create_technical_indicators(self):\n",
    "        \"\"\"Cria indicadores t√©cnicos\"\"\"\n",
    "        print(\"\\nüìà Criando indicadores t√©cnicos...\")\n",
    "        \n",
    "        # Moving Averages\n",
    "        self.df['MA_7'] = self.df['Close'].rolling(window=7).mean()\n",
    "        self.df['MA_21'] = self.df['Close'].rolling(window=21).mean()\n",
    "        self.df['MA_50'] = self.df['Close'].rolling(window=50).mean()\n",
    "        \n",
    "        # Exponential Moving Averages\n",
    "        self.df['EMA_12'] = self.df['Close'].ewm(span=12, adjust=False).mean()\n",
    "        self.df['EMA_26'] = self.df['Close'].ewm(span=26, adjust=False).mean()\n",
    "        \n",
    "        # MACD\n",
    "        self.df['MACD'] = self.df['EMA_12'] - self.df['EMA_26']\n",
    "        self.df['MACD_Signal'] = self.df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "        \n",
    "        # RSI\n",
    "        delta = self.df['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        self.df['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        self.df['BB_Middle'] = self.df['Close'].rolling(window=20).mean()\n",
    "        bb_std = self.df['Close'].rolling(window=20).std()\n",
    "        self.df['BB_Upper'] = self.df['BB_Middle'] + (bb_std * 2)\n",
    "        self.df['BB_Lower'] = self.df['BB_Middle'] - (bb_std * 2)\n",
    "        \n",
    "        # Volume\n",
    "        self.df['Volume_MA_7'] = self.df['Volume'].rolling(window=7).mean()\n",
    "        \n",
    "        # Price changes\n",
    "        self.df['Price_Change'] = self.df['Close'].pct_change()\n",
    "        self.df['High_Low_Pct'] = (self.df['High'] - self.df['Low']) / self.df['Close']\n",
    "        \n",
    "        # Volatility\n",
    "        self.df['Volatility'] = self.df['Close'].rolling(window=20).std()\n",
    "        self.df['ATR'] = self.df['High'] - self.df['Low']\n",
    "        \n",
    "        # Remover NaN\n",
    "        rows_before = len(self.df)\n",
    "        self.df = self.df.dropna()\n",
    "        removed = rows_before - len(self.df)\n",
    "        \n",
    "        print(f\"   ‚úì Indicadores criados: {len(self.df.columns)} colunas\")\n",
    "        print(f\"   ‚úì Removidas {removed} linhas (warm-up)\")\n",
    "        print(f\"   ‚úì Registros finais: {len(self.df)}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def select_features(self):\n",
    "        \"\"\"Seleciona features\"\"\"\n",
    "        self.feature_columns = [\n",
    "            'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "            'MA_7', 'MA_21', 'MA_50',\n",
    "            'EMA_12', 'EMA_26',\n",
    "            'MACD', 'MACD_Signal',\n",
    "            'RSI',\n",
    "            'BB_Upper', 'BB_Middle', 'BB_Lower',\n",
    "            'Volume_MA_7',\n",
    "            'Price_Change', 'High_Low_Pct',\n",
    "            'Volatility', 'ATR'\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüéØ Features selecionadas: {len(self.feature_columns)}\")\n",
    "        for i, feat in enumerate(self.feature_columns, 1):\n",
    "            print(f\"   {i:2d}. {feat}\")\n",
    "        \n",
    "        return self.feature_columns\n",
    "    \n",
    "    def create_sequences(self, df, seq_length=60, target_column='Close'):\n",
    "        \"\"\"Cria sequ√™ncias LSTM\"\"\"\n",
    "        print(f\"\\nüî¢ Criando sequ√™ncias...\")\n",
    "        print(f\"   Lookback: {seq_length} timesteps\")\n",
    "        \n",
    "        data = df.values\n",
    "        target_idx = self.feature_columns.index(target_column)\n",
    "        \n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(seq_length, len(data)):\n",
    "            X.append(data[i-seq_length:i])\n",
    "            y.append(data[i, target_idx])\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        print(f\"   ‚úì X shape: {X.shape}\")\n",
    "        print(f\"   ‚úì y shape: {y.shape}\")\n",
    "        print(f\"   ‚úì Sequ√™ncias: {len(X)}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def split_and_normalize(self, X, y, train_split=0.65, val_split=0.20):\n",
    "        \"\"\"Split temporal + normaliza√ß√£o\"\"\"\n",
    "        print(f\"\\n‚úÇÔ∏è Dividindo dados...\")\n",
    "        \n",
    "        train_size = int(len(X) * train_split)\n",
    "        val_size = int(len(X) * val_split)\n",
    "        \n",
    "        X_train = X[:train_size]\n",
    "        y_train = y[:train_size]\n",
    "        \n",
    "        X_val = X[train_size:train_size+val_size]\n",
    "        y_val = y[train_size:train_size+val_size]\n",
    "        \n",
    "        X_test = X[train_size+val_size:]\n",
    "        y_test = y[train_size+val_size:]\n",
    "        \n",
    "        print(f\"   ‚úì Treino: {len(X_train)} ({train_split*100:.0f}%)\")\n",
    "        print(f\"   ‚úì Val: {len(X_val)} ({val_split*100:.0f}%)\")\n",
    "        print(f\"   ‚úì Teste: {len(X_test)} ({(1-train_split-val_split)*100:.0f}%)\")\n",
    "        \n",
    "        # Normaliza√ß√£o\n",
    "        print(f\"\\nüîÑ Normalizando...\")\n",
    "        print(f\"   ‚úÖ Fit APENAS no treino\")\n",
    "        \n",
    "        n_samples_train, n_timesteps, n_features = X_train.shape\n",
    "        X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "        \n",
    "        self.scaler.fit(X_train_reshaped)\n",
    "        \n",
    "        X_train_scaled = self.scaler.transform(X_train_reshaped).reshape(n_samples_train, n_timesteps, n_features)\n",
    "        X_val_scaled = self.scaler.transform(X_val.reshape(-1, n_features)).reshape(len(X_val), n_timesteps, n_features)\n",
    "        X_test_scaled = self.scaler.transform(X_test.reshape(-1, n_features)).reshape(len(X_test), n_timesteps, n_features)\n",
    "        \n",
    "        # Normalizar y\n",
    "        close_idx = self.feature_columns.index('Close')\n",
    "        \n",
    "        y_train_full = np.zeros((len(y_train), n_features))\n",
    "        y_val_full = np.zeros((len(y_val), n_features))\n",
    "        y_test_full = np.zeros((len(y_test), n_features))\n",
    "        \n",
    "        y_train_full[:, close_idx] = y_train\n",
    "        y_val_full[:, close_idx] = y_val\n",
    "        y_test_full[:, close_idx] = y_test\n",
    "        \n",
    "        y_train_scaled = self.scaler.transform(y_train_full)[:, close_idx]\n",
    "        y_val_scaled = self.scaler.transform(y_val_full)[:, close_idx]\n",
    "        y_test_scaled = self.scaler.transform(y_test_full)[:, close_idx]\n",
    "        \n",
    "        print(f\"   ‚úÖ Normaliza√ß√£o completa\")\n",
    "        \n",
    "        return (X_train_scaled, y_train_scaled), (X_val_scaled, y_val_scaled), (X_test_scaled, y_test_scaled)\n",
    "    \n",
    "    def save_preprocessor(self, filename='preprocessor.pkl'):\n",
    "        \"\"\"Salva preprocessor\"\"\"\n",
    "        filepath = os.path.join('/kaggle/working/', filename)\n",
    "        \n",
    "        preprocessor_data = {\n",
    "            'scaler': self.scaler,\n",
    "            'feature_columns': self.feature_columns\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(preprocessor_data, f)\n",
    "        \n",
    "        print(f\"\\nüíæ Preprocessor salvo: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Pipeline completo\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PR√â-PROCESSAMENTO - 100% LIVRE DE DATA LEAKAGE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Configura√ß√£o\n",
    "    INPUT_FILE = 'spy_raw.csv'\n",
    "    \n",
    "    # Carregar\n",
    "    print(f\"\\nüìÇ Carregando: /kaggle/working/{INPUT_FILE}\")\n",
    "    data_path = os.path.join('/kaggle/working/', INPUT_FILE)\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Arquivo n√£o encontrado: {data_path}\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(data_path, index_col=0, parse_dates=True)\n",
    "    print(f\"   ‚úì Carregados {len(df)} registros\")\n",
    "    \n",
    "    # Preprocessor\n",
    "    preprocessor = StockDataPreprocessor(df)\n",
    "    \n",
    "    # Pipeline\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXECUTANDO PIPELINE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    preprocessor.convert_to_numeric()  # ‚Üê NOVA LINHA!\n",
    "    preprocessor.handle_missing_values()\n",
    "    preprocessor.create_technical_indicators()\n",
    "    preprocessor.select_features()\n",
    "    \n",
    "    df_features = preprocessor.df[preprocessor.feature_columns]\n",
    "    \n",
    "    X, y = preprocessor.create_sequences(df_features, seq_length=60)\n",
    "    \n",
    "    train_data, val_data, test_data = preprocessor.split_and_normalize(\n",
    "        X, y, train_split=0.65, val_split=0.20\n",
    "    )\n",
    "    \n",
    "    # Salvar\n",
    "    print(f\"\\nüíæ Salvando dados...\")\n",
    "    np.save('/kaggle/working/X_train.npy', train_data[0])\n",
    "    np.save('/kaggle/working/y_train.npy', train_data[1])\n",
    "    np.save('/kaggle/working/X_val.npy', val_data[0])\n",
    "    np.save('/kaggle/working/y_val.npy', val_data[1])\n",
    "    np.save('/kaggle/working/X_test.npy', test_data[0])\n",
    "    np.save('/kaggle/working/y_test.npy', test_data[1])\n",
    "    print(f\"   ‚úì 6 arquivos .npy salvos\")\n",
    "    \n",
    "    preprocessor.save_preprocessor()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"‚úÖ PR√â-PROCESSAMENTO CONCLU√çDO\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nüìã Checklist:\")\n",
    "    print(f\"   ‚úÖ Convers√£o para num√©rico\")\n",
    "    print(f\"   ‚úÖ Missing values (forward fill)\")\n",
    "    print(f\"   ‚úÖ Indicadores (janela passada)\")\n",
    "    print(f\"   ‚úÖ Sequ√™ncias (sem leakage)\")\n",
    "    print(f\"   ‚úÖ Split (temporal)\")\n",
    "    print(f\"   ‚úÖ Normaliza√ß√£o (fit no treino)\")\n",
    "    \n",
    "    return preprocessor, train_data, val_data, test_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a6108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T05:39:44.219917Z",
     "iopub.status.busy": "2026-02-09T05:39:44.219649Z",
     "iopub.status.idle": "2026-02-09T05:39:44.224681Z",
     "shell.execute_reply": "2026-02-09T05:39:44.224024Z",
     "shell.execute_reply.started": "2026-02-09T05:39:44.219893Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verificar GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Se quiser for√ßar uso de GPU\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"‚úÖ GPU detectada! TensorFlow usar√° GPU automaticamente.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhuma GPU detectada. Usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044c39a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T05:39:44.225966Z",
     "iopub.status.busy": "2026-02-09T05:39:44.225769Z",
     "iopub.status.idle": "2026-02-09T05:40:18.315867Z",
     "shell.execute_reply": "2026-02-09T05:40:18.315200Z",
     "shell.execute_reply.started": "2026-02-09T05:39:44.225947Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "========================================\n",
    "SCRIPT 3: MODELAGEM LSTM\n",
    "Vers√£o Final - Treinamento e Avalia√ß√£o\n",
    "========================================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Seed para reprodutibilidade\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "class LSTMStockPredictor:\n",
    "    \"\"\"Modelo LSTM para predi√ß√£o de a√ß√µes\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def build_model(self, lstm_units=[128, 64, 32], dropout_rate=0.2, learning_rate=0.001):\n",
    "        \"\"\"Constr√≥i arquitetura LSTM\"\"\"\n",
    "        print(\"\\nüèóÔ∏è Construindo modelo LSTM...\")\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Primeira LSTM (retorna sequ√™ncias)\n",
    "        model.add(LSTM(units=lstm_units[0], return_sequences=True, input_shape=self.input_shape))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        # LSTMs intermedi√°rias\n",
    "        for units in lstm_units[1:-1]:\n",
    "            model.add(LSTM(units=units, return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        # √öltima LSTM (n√£o retorna sequ√™ncias)\n",
    "        model.add(LSTM(units=lstm_units[-1], return_sequences=False))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        # Dense layers\n",
    "        model.add(Dense(units=25, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(units=1))\n",
    "        \n",
    "        # Compilar\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(f\"   ‚úì Modelo constru√≠do!\")\n",
    "        print(f\"\\nüìã Arquitetura:\")\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32, verbose=1):\n",
    "        \"\"\"Treina o modelo\"\"\"\n",
    "        print(\"\\nüéì Iniciando treinamento...\")\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            '/kaggle/working/best_model.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Treinar\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        self.history = history\n",
    "        print(\"\\n‚úÖ Treinamento conclu√≠do!\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, scaler, feature_columns):\n",
    "        \"\"\"Avalia o modelo e calcula m√©tricas\"\"\"\n",
    "        print(\"\\nüìä Avaliando modelo...\")\n",
    "        \n",
    "        # Predi√ß√µes\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Desnormalizar\n",
    "        close_idx = feature_columns.index('Close')\n",
    "        \n",
    "        y_test_full = np.zeros((len(y_test), len(feature_columns)))\n",
    "        y_pred_full = np.zeros((len(y_pred), len(feature_columns)))\n",
    "        \n",
    "        y_test_full[:, close_idx] = y_test\n",
    "        y_pred_full[:, close_idx] = y_pred.flatten()\n",
    "        \n",
    "        y_test_original = scaler.inverse_transform(y_test_full)[:, close_idx]\n",
    "        y_pred_original = scaler.inverse_transform(y_pred_full)[:, close_idx]\n",
    "        \n",
    "        # M√©tricas\n",
    "        mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "        mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100\n",
    "        \n",
    "        print(f\"\\nüìà M√âTRICAS DE AVALIA√á√ÉO:\")\n",
    "        print(f\"   MAE (Mean Absolute Error): ${mae:.4f}\")\n",
    "        print(f\"   RMSE (Root Mean Square Error): ${rmse:.4f}\")\n",
    "        print(f\"   MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            'y_test': y_test_original,\n",
    "            'y_pred': y_pred_original,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape\n",
    "        }\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plota hist√≥rico de treinamento\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"‚ö†Ô∏è Modelo ainda n√£o foi treinado!\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss\n",
    "        ax1.plot(self.history.history['loss'], label='Treino')\n",
    "        ax1.plot(self.history.history['val_loss'], label='Valida√ß√£o')\n",
    "        ax1.set_title('Loss durante Treinamento')\n",
    "        ax1.set_xlabel('√âpoca')\n",
    "        ax1.set_ylabel('Loss (MSE)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # MAE\n",
    "        ax2.plot(self.history.history['mae'], label='Treino')\n",
    "        ax2.plot(self.history.history['val_mae'], label='Valida√ß√£o')\n",
    "        ax2.set_title('MAE durante Treinamento')\n",
    "        ax2.set_xlabel('√âpoca')\n",
    "        ax2.set_ylabel('MAE')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/kaggle/working/training_history.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"üíæ Gr√°fico salvo: /kaggle/working/training_history.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_predictions(self, evaluation_results, num_points=500):\n",
    "        \"\"\"Plota predi√ß√µes vs valores reais\"\"\"\n",
    "        y_test = evaluation_results['y_test'][-num_points:]\n",
    "        y_pred = evaluation_results['y_pred'][-num_points:]\n",
    "        \n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(y_test, label='Valor Real', linewidth=2, alpha=0.8)\n",
    "        plt.plot(y_pred, label='Predi√ß√£o', linewidth=2, alpha=0.8)\n",
    "        plt.title(f'Predi√ß√£o vs Valor Real (√∫ltimos {num_points} pontos)', fontsize=14)\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Pre√ßo de Fechamento ($)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/kaggle/working/predictions_vs_real.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"üíæ Gr√°fico salvo: /kaggle/working/predictions_vs_real.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_error_distribution(self, evaluation_results):\n",
    "        \"\"\"Plota distribui√ß√£o dos erros\"\"\"\n",
    "        y_test = evaluation_results['y_test']\n",
    "        y_pred = evaluation_results['y_pred']\n",
    "        errors = y_test - y_pred\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Histograma\n",
    "        ax1.hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "        ax1.set_title('Distribui√ß√£o dos Erros')\n",
    "        ax1.set_xlabel('Erro ($)')\n",
    "        ax1.set_ylabel('Frequ√™ncia')\n",
    "        ax1.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Scatter\n",
    "        ax2.scatter(y_test, y_pred, alpha=0.5)\n",
    "        ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', linewidth=2, label='Predi√ß√£o Perfeita')\n",
    "        ax2.set_title('Predi√ß√£o vs Valor Real')\n",
    "        ax2.set_xlabel('Valor Real ($)')\n",
    "        ax2.set_ylabel('Predi√ß√£o ($)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/kaggle/working/error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"üíæ Gr√°fico salvo: /kaggle/working/error_analysis.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def save_model(self, filename='lstm_model.keras'):\n",
    "        \"\"\"Salva modelo\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"‚ö†Ô∏è Nenhum modelo para salvar!\")\n",
    "            return\n",
    "        \n",
    "        filepath = os.path.join('/kaggle/working/', filename)\n",
    "        self.model.save(filepath)\n",
    "        print(f\"\\nüíæ Modelo salvo: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Executa treinamento completo\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODELAGEM LSTM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Carregar dados\n",
    "    print(\"\\nüìÇ Carregando dados pr√©-processados...\")\n",
    "    \n",
    "    try:\n",
    "        X_train = np.load('/kaggle/working/X_train.npy')\n",
    "        y_train = np.load('/kaggle/working/y_train.npy')\n",
    "        X_val = np.load('/kaggle/working/X_val.npy')\n",
    "        y_val = np.load('/kaggle/working/y_val.npy')\n",
    "        X_test = np.load('/kaggle/working/X_test.npy')\n",
    "        y_test = np.load('/kaggle/working/y_test.npy')\n",
    "        \n",
    "        with open('/kaggle/working/preprocessor.pkl', 'rb') as f:\n",
    "            preprocessor_data = pickle.load(f)\n",
    "        \n",
    "        scaler = preprocessor_data['scaler']\n",
    "        feature_columns = preprocessor_data['feature_columns']\n",
    "        \n",
    "        print(f\"   ‚úì Dados carregados!\")\n",
    "        print(f\"   ‚úì X_train: {X_train.shape}\")\n",
    "        print(f\"   ‚úì X_val: {X_val.shape}\")\n",
    "        print(f\"   ‚úì X_test: {X_test.shape}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Erro: {e}\")\n",
    "        print(\"   Execute o script de pr√©-processamento primeiro!\")\n",
    "        return None\n",
    "    \n",
    "    # Criar modelo\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    predictor = LSTMStockPredictor(input_shape)\n",
    "    \n",
    "    # Construir\n",
    "    predictor.build_model(\n",
    "        lstm_units=[128, 64, 32],\n",
    "        dropout_rate=0.2,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    # Treinar\n",
    "    history = predictor.train(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Visualizar hist√≥rico\n",
    "    predictor.plot_training_history()\n",
    "    \n",
    "    # Avaliar\n",
    "    evaluation_results = predictor.evaluate(X_test, y_test, scaler, feature_columns)\n",
    "    \n",
    "    # Visualizar predi√ß√µes\n",
    "    predictor.plot_predictions(evaluation_results, num_points=500)\n",
    "    predictor.plot_error_distribution(evaluation_results)\n",
    "    \n",
    "    # Salvar modelo\n",
    "    predictor.save_model('lstm_stock_model.keras')\n",
    "    \n",
    "    # Salvar m√©tricas\n",
    "    metrics_df = pd.DataFrame([{\n",
    "        'MAE': evaluation_results['mae'],\n",
    "        'RMSE': evaluation_results['rmse'],\n",
    "        'MAPE': evaluation_results['mape'],\n",
    "        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }])\n",
    "    metrics_df.to_csv('/kaggle/working/model_metrics.csv', index=False)\n",
    "    print(f\"üíæ M√©tricas salvas: /kaggle/working/model_metrics.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ MODELAGEM COMPLETA!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüìÅ Arquivos gerados:\")\n",
    "    print(f\"   ‚Ä¢ lstm_stock_model.keras\")\n",
    "    print(f\"   ‚Ä¢ best_model.keras\")\n",
    "    print(f\"   ‚Ä¢ training_history.png\")\n",
    "    print(f\"   ‚Ä¢ predictions_vs_real.png\")\n",
    "    print(f\"   ‚Ä¢ error_analysis.png\")\n",
    "    print(f\"   ‚Ä¢ model_metrics.csv\")\n",
    "    \n",
    "    return predictor, evaluation_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f4f56",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.113606,
   "end_time": "2026-02-10T02:37:18.736096",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-10T02:37:13.622490",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
